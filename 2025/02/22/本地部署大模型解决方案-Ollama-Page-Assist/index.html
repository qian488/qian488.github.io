
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <title>本地部署大模型解决方案( Ollama + Page Assist) | 千幻笙的小窝</title>
    <meta name="author" content="Awith" />
    <meta name="description" content="" />
    <meta name="keywords" content="Awith's blogs,Awith's algorithm blogs,Awith's home" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <link rel="icon" href="../../../../images/avatar.jpg" />
    <link rel="preconnect" href="https://cdn.staticfile.org" />
<script src="https://cdn.staticfile.org/vue/3.3.7/vue.global.prod.min.js"></script>
<script src="/live2d-widget/autoload.js"></script>
<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.loli.net" />
<link rel="preconnect" href="https://gstatic.loli.net" crossorigin />
<link rel="stylesheet" href="https://fonts.loli.net/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap" />
<script> const mixins = {}; </script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=default"></script>


<script src="https://cdn.staticfile.org/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.org/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://cdn.staticfile.org/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="../../../../js/lib/highlight.js"></script>



<script src="../../../../js/lib/preview.js"></script>









<link rel="stylesheet" href="../../../../css/main.css" />

<meta name="generator" content="Hexo 7.3.0"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="../../../../images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>千幻笙的小窝</span>
        </a>
        
        <a href="../../../../index.html">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="../../../../about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="../../../../archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="../../../../categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="../../../../tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;千幻笙的小窝</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="../../../../index.html">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="../../../../about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="../../../../archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="../../../../categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="../../../../tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div class="article">
    <div>
        <h1>本地部署大模型解决方案( Ollama + Page Assist)</h1>
    </div>
    <div class="info">
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/2/22
        </span>
        
        <span class="category">
            <a href="../../../../categories/%E5%88%86%E4%BA%AB/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                分享
            </a>
        </span>
        
        
        <span class="tags">
            <span class="icon">
                <i class="fa-solid fa-tags fa-fw"></i>
            </span>
            
            
            <span class="tag">
                
                <a href="../../../../tags/ollama/" style="color: #ff7d73">ollama</a>
            </span>
            
            <span class="tag">
                
                <a href="../../../../tags/deepseek/" style="color: #03a9f4">deepseek</a>
            </span>
            
        </span>
        
    </div>
    
    <div class="content" v-pre>
        <h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>最近，deepseek r1的大模型比较火，然后，官方的经常会无法响应，可能是用的人太多了。但是，它胜在开源，并且发布了基于r1模型蒸馏后的小模型。于是，本地部署deepseek的方案就诞生了。经过我的一番折腾，我的本地部署大模型的解决方案就决定是ollama+page assist.</p>
<p>ollama是一个便捷的大模型管理器，page assist是chrome插件。</p>
<span id="more"></span>

<h3 id="安装ollama"><a href="#安装ollama" class="headerlink" title="安装ollama"></a>安装ollama</h3><p><a target="_blank" rel="noopener" href="https://ollama.com/">官方下载地址</a></p>
<p>下载ollama的安装包，先别急着安装。因为它默认是安装在c盘的，怕你c盘不够用。我是装在E盘，考虑到不是都会分出有E盘，这里以装在D盘为例。</p>
<p>例如我现在想把ollama安装在D:\Ollama路径下，以Windows系统为例</p>
<ul>
<li>1、把下载好的安装包（也就是OllamaSetup.exe）放到D:\Ollama路径下</li>
</ul>
<p><img src="https://raw.githubusercontent.com/qian488/qian_personal_bolg_images/main/img/image-20250222134716318.png" alt="image-20250222134716318"></p>
<ul>
<li>2、打开下载安装包的文件夹，在地址栏(上图蓝色那块就是)输入cmd打开</li>
</ul>
<p><img src="https://raw.githubusercontent.com/qian488/qian_personal_bolg_images/main/img/image-20250222134805483.png" alt="image-20250222134805483"></p>
<ul>
<li>3、输入<code>OllamaSetup.exe /DIR=D:\Ollama</code>就能跳转到ollama的安装界面</li>
</ul>
<p>然后傻瓜式一路下一步就能安装完成了。因为我已经安装了，就不再进行安装演示。</p>
<p>cmd窗口输入ollama，出现如下图的信息，即为安装成功。</p>
<p><img src="https://raw.githubusercontent.com/qian488/qian_personal_bolg_images/main/img/image-20250222134327473.png" alt="image-20250222134327473"></p>
<h3 id="下载deekspeek模型"><a href="#下载deekspeek模型" class="headerlink" title="下载deekspeek模型"></a>下载deekspeek模型</h3><p>还是ollama官网，点击deepseek r1，然后可以选择对应的模型</p>
<p><img src="https://raw.githubusercontent.com/qian488/qian_personal_bolg_images/main/img/image-20250222135216896.png" alt="image-20250222135216896"></p>
<p>先别急着下载，模型很可能直接下到C盘的。</p>
<p>想将模型下载到 <code>D</code> 盘，可以通过以下方法实现：</p>
<p>通过设置环境变量 <code>OLLAMA_MODELS</code>，可以指定 Ollama 存储模型的路径。</p>
<ol>
<li><strong>打开环境变量设置</strong>：</li>
</ol>
<ul>
<li>右键点击“此电脑”或“我的电脑”，选择“属性”。 </li>
<li>点击“高级系统设置”。 </li>
<li>在“系统属性”窗口中，点击“环境变量”。</li>
</ul>
<ol start="2">
<li><strong>新建系统变量</strong>：</li>
</ol>
<ul>
<li>在“系统变量”部分，点击“新建”。</li>
<li>输入以下内容：<br><strong>变量名</strong>：<code>OLLAMA_MODELS</code><br><strong>变量值</strong>：<code>D:\ollama_models</code>（或你希望存储模型的路径）</li>
</ul>
<p>然后在进行模型的选择。模型的选择根据你电脑的显存和内存进行。尽量将模型完全放在显存去跑。例如我的是16G显存的4060ti和32G内存，所以可以选择32b的模型运行，但是因为不是完全在显存跑，所以回复的速度会很慢。所以我又选择了14b的模型进行下载。这里推荐再下载一个1.5b的模型，模型小，虽然回复的质量不行，但是需要的硬件资源小，在进行本地ai相关应用开发的时候使用还是很不错的。</p>
<p>选好后复制旁边的命令，在cmd，就会进行模型的下载，并且启动。</p>
<p>出现以下界面的时候，就代表模型安装好了。</p>
<p><img src="https://raw.githubusercontent.com/qian488/qian_personal_bolg_images/main/img/image-20250222141052520.png" alt="image-20250222141052520"></p>
<p>就可以与本地的ai进行chat</p>
<h3 id="安装Page-Assist"><a href="#安装Page-Assist" class="headerlink" title="安装Page Assist"></a>安装Page Assist</h3><p>打开谷歌浏览器，然后去插件商店下载扩展即可。</p>
<p>打开插件的页面，就摆脱了终端窗口，有了ui界面。</p>
<p><img src="https://raw.githubusercontent.com/qian488/qian_personal_bolg_images/main/img/image-20250222141432213.png" alt="image-20250222141432213"></p>
<p>选择模型，然后进行问答。</p>
<p>至此，你已经拥有了一个本地部署的大模型。</p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>以下是ollama官方仓库对api调用模型的介绍。</p>
<p><strong>REST API</strong></p>
<p>Ollama has a REST API for running and managing models.</p>
<p><strong>Generate a response</strong></p>
<pre><code>curl http://localhost:11434/api/generate -d &#39;&#123;
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;prompt&quot;:&quot;Why is the sky blue?&quot;
&#125;&#39;
</code></pre>
<p><strong>Chat with a model</strong></p>
<pre><code>curl http://localhost:11434/api/chat -d &#39;&#123;
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;messages&quot;: [
    &#123; &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;why is the sky blue?&quot; &#125;
  ]
&#125;&#39;
</code></pre>
<p>See the <a target="_blank" rel="noopener" href="https://github.com/ollama/ollama/blob/main/docs/api.md">API documentation</a> for all endpoints.</p>
<p>如果想在自己程序中调用本地的大模型生成答案，可参考我用ai使用python写的一个代码。</p>
<pre><code class="python">import requests
import json
from config import Config
from logger import get_logger

class ModelClient:
    def __init__(self, endpoint: str = Config.OLLAMA_ENDPOINT, model: str = Config.LLM_MODEL):
        &quot;&quot;&quot;
        初始化 ModelClient。
        :param endpoint: Ollama 服务的地址。
        :param model: 使用的模型名称。
        &quot;&quot;&quot;
        self.endpoint = endpoint
        self.model = model
        self.logger = get_logger(&quot;ModelClient&quot;)  # 获取日志记录器
        self.logger.info(f&quot;模型客户端初始化完成。Endpoint: &#123;self.endpoint&#125;, Model: &#123;self.model&#125;&quot;)

    def generate_answer(self, prompt: str) -&gt; str:
        &quot;&quot;&quot;
        使用 Ollama 模型生成回答。
        :param prompt: 用户输入的问题。
        :return: 模型的完整回答。
        &quot;&quot;&quot;
        self.logger.info(f&quot;请求模型回答，问题: &#123;prompt&#125;&quot;)
        try:
            response = requests.post(
                self.endpoint,
                json=&#123;&quot;model&quot;: self.model, &quot;prompt&quot;: prompt&#125;,
                stream=True  # 使用流式响应
            )
            response.raise_for_status()  # 检查请求是否成功

            # 初始化完整回答
            full_answer = &quot;&quot;

            # 遍历流式响应
            for line in response.iter_lines():
                if line:
                    decoded_line = line.decode(&quot;utf-8&quot;)
                    # self.logger.debug(f&quot;接收到响应行: &#123;decoded_line&#125;&quot;)
                    try:
                        data = json.loads(decoded_line)
                        if &quot;response&quot; in data:
                            # 拼接 response 内容
                            full_answer += data[&quot;response&quot;]
                            print(data[&quot;response&quot;], end=&quot;&quot;, flush=True)  # 实时输出
                        if data.get(&quot;done&quot;, False):  # 检查是否完成
                            self.logger.info(&quot;模型返回完成标志，结束拼接。&quot;)
                            break
                    except json.JSONDecodeError:
                        self.logger.error(f&quot;解析响应行时出错: &#123;decoded_line&#125;&quot;)
                        continue

            return full_answer  # 返回完整回答
        except requests.exceptions.RequestException as e:
            self.logger.error(f&quot;请求模型时发生错误：&#123;e&#125;&quot;)
            return f&quot;请求模型时发生错误：&#123;e&#125;&quot;
</code></pre>
<p>其中，配置中的内容为，</p>
<pre><code class="python">OLLAMA_ENDPOINT = &quot;http://127.0.0.1:11434/api/generate&quot;
LLM_MODEL = &quot;deepseek-r1:14b&quot;
</code></pre>
<p>其他我暂时也还没有探索。</p>
<p>也可参考使用C#，导入对应的包直接开箱即用</p>
<pre><code class="csharp">using Microsoft.Extensions.AI;
using OllamaSharp;
using System.Net.Mail;

namespace OllamaApiTest
&#123;
    class Program
    &#123;
        static async Task Main(string[] args)
        &#123;
            Uri modelEndpoint = new(&quot;http://localhost:11434&quot;);

            //指定模型
            string modelName = &quot;deepseek-r1:14b&quot;;

            var chatClient = new OllamaApiClient(modelEndpoint, modelName);

            while (true)
            &#123;
                Console.Write(&quot;&gt;&gt;&gt; 请输入您的问题：&quot;);
                //提问
                string? question = Console.ReadLine();

                if (question == &quot;exit&quot;) break;
                if (!string.IsNullOrEmpty(question))
                &#123;
                    var response = chatClient.GetStreamingResponseAsync(question);

                    Console.WriteLine($&quot;&gt;&gt;&gt; 你: &#123;question&#125;&quot;);
                    Console.Write(&quot;&gt;&gt;&gt; &quot;);
                    Console.WriteLine(&quot;&gt;&gt;&gt; DeepSeek: &quot;);

                    //输出
                    await foreach (var item in response)
                    &#123;
                        Console.Write(item);
                    &#125;

                    Console.WriteLine();
                &#125;
            &#125;
        &#125;
    &#125;
&#125;
</code></pre>

    </div>
    
    
    
    
    
    
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2022 - 2025 千幻笙的小窝
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;Awith
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="../../../../js/main.js"></script>
    
    




    
</body>
</html>
